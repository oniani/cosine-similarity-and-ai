%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Filename: template.tex
% Author:   David Oniani
% Modified: April 19, 2020
%  _         _____   __  __
% | |    __ |_   _|__\ \/ /
% | |   / _` || |/ _ \\  /
% | |__| (_| || |  __//  \
% |_____\__,_||_|\___/_/\_\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document Definition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages and Related Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Global, document-wide settings
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% Bibliography and references
\usepackage[backend=biber,sorting=none]{biblatex}
\addbibresource{references.bib}

% Other packages
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[cache=false]{minted}
\usepackage{tocloft}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Command Definitions and Redefinitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Operators
\DeclarePairedDelimiter\abs{\lvert}{\rvert}               % Absolute value
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}              % Ceiling
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}           % Floor

% New commands
\newcommand\und[1]{\underline{\smash{#1}}}  % Nice-looking underline
\renewcommand{\baselinestretch}{1.5}        % Line spacing is 1.5

% Rename "Contents" to "Table of Contents"
\addto\captionsenglish{% Replace "english" with the language used
  \renewcommand{\contentsname}%
    {\textbf{Table of Contents}}}%

% Filling the space for centering the title of the table of contents
% Dots for ToC sections
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cfttoctitlefont}{\hspace*{\fill}\Large}
\renewcommand{\cftaftertoctitle}{\hspace*{\fill}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Miscellaneous
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Setting stuff
\setlength{\parindent}{0pt}  % Remove indentations from paragraphs
\pagestyle{fancy}            % This allows to do fancy headers and footers
\fancyhf{}                   % No additional page numbering (or other stuff)
\cfoot{\thepage}             % Display page number at the bottom, in the center
\usemintedstyle{tango}       % Colorscheme for minted

% PDF information and nice-looking urls
\hypersetup{%
  pdfauthor={David Oniani},
  pdftitle={Your Title Goes Here},
  pdfsubject={Subject 1, Subject 2, and Subject 3},
  pdfkeywords={Keyword 1, Keyword 2, Keyword 3},
  pdflang={English},
  colorlinks=true,
  linkcolor={black!50!blue},
  citecolor={black!50!blue},
  urlcolor={black!50!blue}
}

% Put a header
\chead{\footnotesize{Cosine Similarity and Its Applications in AI}}
\lhead{\footnotesize{David Oniani}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author(s), Title, and Date
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Author(s)
\author{David Oniani\\
        Luther College\\
        \href{mailto:oniada01@luther.edu}{oniada01@luther.edu}}

% Title
\title{\textbf{Cosine Similarity and Its Applications in AI}}

% Date
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of the Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
  \noindent Choosing the right metric~\cite{thomas2020} can be crucial to
  designing performant artificial intelligence models. Thousands of packages
  and libraries have been built and written just for providing these metrics.
  Cosine similarity is one of many metrics used extensively in natural language
  processing and artificial intelligence tasks. The paper will introduce the
  technique and discuss its advantages and disadvantages as well as compare it
  to other approaches. Additionally, sample implementations of the
  above-mentioned approaches will also be provided.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

There are many approaches for determining whether two texts are semantically
similar to each other. Cosine similarity is one of those methods. Initially
derived from the law of cosines~\cite{wikicosineproof}, the technique is now
widely used in natural language processing (and other) problems.

\bigskip

In order to understand how cosine similarity works, let us first discuss how
a text document can be modeled.

\subsection{Text Document Modeling}

There are several ways in which a text document can be modeled. This includes a
\textit{bag of words} modeling, where the frequency of a term in a text
document represents its weight and therefore, more frequent words are deemed
more ``important.'' Another example is \textit{tf-idf} vectorization. The
primary goal of text document modeling is to transform the textual data into
the numeric data (usually, term vectors). Once the numeric data is obtained, we
can then apply various text semantic similarity techniques in order to compare
if two documents are similar to each other.

\subsection{Cosine Similarity}

Cosine similarity is a measure of similarity between two non-zero vectors of an
inner product space that measures the cosine of the angle between
them~\cite{wikicosine}. If we have two vectors \(\vec{a}\) and \(\vec{b}\),
then the cosine of these two vectors is
\(\vec{a} \cdot \vec{b} = \abs{\vec{a}}\abs{\vec{b}}\cos\theta\),
where \(\theta\) is the angle between the vectors. This is also known as the
Euclidean dot product formula.
The cosine of two vectors can also be computed using their coordinates:
if \(\vec{a} = \begin{bmatrix}a_1 \\ a_2\end{bmatrix}\) and
\(\vec{b} = \begin{bmatrix}b_1 \\ b_2\end{bmatrix}\), then
\(\vec{a} \cdot \vec{b} = a_1b_1 + a_2b_2\). Hence, for two vectors \(\vec{a}\)
and \(\vec{b}\) with coordinates \(a_1, a_2\) and \(b_1, b_2\) respectively,
the following holds:

\[\vec{a} \cdot \vec{b} = \abs{\vec{a}}\abs{\vec{b}}\cos\theta = a_1b_1 + a_2b_2\]

\bigskip

If the vector coordinates are known, then one can also compute the magnitude of
the vector. For vector
\(\vec{x} = \begin{bmatrix}x_1 \\ x_2 \\ \dots \\ x_n\end{bmatrix}\),
the magnitude is \(\sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}\). Hence, if we have two
vectors \(\vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ \dots \\ a_n\end{bmatrix}\) and
\(\vec{b} = \begin{bmatrix}b_1 \\ b_2 \\ \dots \\ b_n\end{bmatrix}\), then the cosine
of an angle between them can be computed using the formula:

\[ \cos\theta = \dfrac{a_1b_1 + a_2b_2 + \cdots + a_nb_n}{\sqrt{a_1^2 + a_2^2 + \cdots + a_n^2}\sqrt{b_1^2 + b_2^2 + \cdots + b_n^2}} \]

In other words, if we have two vectors with all the coordinates, it is possible
to compute the cosine similarity between them.

\subsection{Example}

As an example, consider two vectors
\(\vec{a} = \begin{bmatrix}3 \\ 4\end{bmatrix}\)
and \(\vec{b} = \begin{bmatrix}5 \\ 12\end{bmatrix}\). Then the magnitude of \(a\) is
\(\sqrt{3^2 + 4^2} = 5\) and the magnitude of \(b\) is \(\sqrt{5^2 + 12^2} = 13\).
It follows that \(\vec{a} \cdot \vec{b} = 15 + 48 = 63\) and
\(\abs{\vec{a}} \times \abs{\vec{b}} = 5 * 13 = 65\). Therefore, the cosine of
an angle between the vectors is \(\dfrac{63}{65} \approx 0.969\) which is the
similarity measure between these two vectors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Applications
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Applications}

Cosine similarity is one of the most commonly used approaches in calculating
semantic similarity across the documents. Therefore, it is naturally employed
in natural language processing tasks. Many NLP applications need to compute the
similarity in meaning between two short texts. Search engines, for instance,
need to model the relevance of a document to a query, beyond the overlap in
words between the two. Similarly, Q\&A websites such as Quora need to determine
whether a question has already been asked before. This type of text similarity
is often computed by first embedding the two short texts and then calculating
the cosine similarity between them.

\bigskip

Suppose that we have two documents \(D_1\) and \(D_2\) modeled as term vectors
\(\vec{t_1}\) and \(\vec{t_2}\) respectively. Then the similarity of two
documents corresponds to the correlation between the vectors and can be
quantified as a cosine of the angle between the vectors. The cosine similarity
formula~\cite{huang2008} would be:

\[SIM_C(\vec{t_1}, \vec{t_2}) = \dfrac{\vec{t_1} \cdot
  \vec{t_2}}{\abs{\vec{t_1}} \times \abs{\vec{t_2}}}.\]

As a result, the similarity value is non-negative, bounded by the closed
interval \([0,1]\).

\bigskip

It is important to note that the metric is a measurement of orientation and not
the magnitude. It is really a comparison of documents on a normalized space
since it does not only consider the magnitude of word counts of each document,
but also the angle between the documents.

\bigskip

Below find a Python implementation of cosine similarity.

\begin{figure}[H]
  \begin{minted}[fontsize=\small,frame=single]{python}
  import numpy as np

  def cosine_similarity(a: np.array, b: np.array) -> float:
      """Returns the cosine similarity value of two vectors."""

      dot_product: float = np.dot(a, b)
      magnitude_a: float = np.linalg.norm(a)
      magnitude_b: float = np.linalg.norm(b)
      similarity: float = dot_product / (magnitude_a * magnitude_b)

      return similarity


  if __name__ == "__main__":
      a: np.array = np.array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1])
      b: np.array = np.array([0, 2, 1, 0, 1, 3, 0, 0, 0, 1, 0])
      print(cosine_similarity(a, b))  # Prints out 0.5
  \end{minted}
  \caption{Sample Python Implementation of Cosine Similarity.}
\end{figure}

The program above is fairly simplistic (yet performant) and as shown, it takes
two vectors and prints out the cosine similarity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comparison to Other Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Performance and Comparison to Other Approaches}

Performance of cosine similarity can be altered by the apparoch used for
vectorization~\cite{sitikhu2019} (e.g., tf-idf vectorization might yield better
results than Word2Vec vectorization). This means that the right vectorization
strategy could drastically increase the performance, while the poor
vectorization will, unsurprisingly, result in the poor performance.

\medskip

There are many other approaches for finding semantic similarity between the
texts. The recent advancements in the domains of artificial intelligence and
natural language processing allowed for development of models such as
BERT~\cite{turc2019}, BioBERT~\cite{btz682}, and Universal Sentence Encoder
(USE)~\cite{use}, all of which can be used for high-accuracy text semantic
similarity score computations. The advantage of the cosine similarity, however,
is its simplicity and the fact that one could run it over massive datasets.
Doing the same with models like BioBERT could possibly take a lot time and
hence, be highly inefficient.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Current Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Current Work}

Cosine similarity is very much used in the modern, state-of-the-art papers,
such as ones cited in this paper. Its flexibility allows one to apply it
virtually under any setting, as long as documents can be represented as
vectors. Besides, it is widely used for benchmarking purposes and is usually a
standard against which the new approaches are usually.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Summary
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

We have introduced cosine similarity and walked through examples to strenghten
the understanding of the concept. We have discussed the importance of cosine
similarity in the domains of artificial intelligence and natural language
processing and its prevalence in the state-of-the-art work. Sample Python
implementation of the algorithm was also provided.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography and References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography[heading=bibintoc]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The End of the Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
