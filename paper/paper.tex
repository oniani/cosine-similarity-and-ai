%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Filename: template.tex
% Author:   David Oniani
% Modified: April 19, 2020
%  _         _____   __  __
% | |    __ |_   _|__\ \/ /
% | |   / _` || |/ _ \\  /
% | |__| (_| || |  __//  \
% |_____\__,_||_|\___/_/\_\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document Definition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages and Related Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Global, document-wide settings
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% Bibliography and references
\usepackage[backend=biber,sorting=none]{biblatex}
\addbibresource{references.bib}

% Other packages
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[cache=false]{minted}
\usepackage{tocloft}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Command Definitions and Redefinitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Operators
\DeclarePairedDelimiter\abs{\lvert}{\rvert}               % Absolute value
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}              % Ceiling
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}           % Floor

% New commands
\newcommand\und[1]{\underline{\smash{#1}}}  % Nice-looking underline
\renewcommand{\baselinestretch}{1.5}        % Line spacing is 1.5

% Rename "Contents" to "Table of Contents"
\addto\captionsenglish{% Replace "english" with the language used
  \renewcommand{\contentsname}%
    {\textbf{Table of Contents}}}%

% Filling the space for centering the title of the table of contents
% Dots for ToC sections
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cfttoctitlefont}{\hspace*{\fill}\Large}
\renewcommand{\cftaftertoctitle}{\hspace*{\fill}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Miscellaneous
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Setting stuff
\setlength{\parindent}{0pt}  % Remove indentations from paragraphs
\pagestyle{fancy}            % This allows to do fancy headers and footers
\fancyhf{}                   % No additional page numbering (or other stuff)
\cfoot{\thepage}             % Display page number at the bottom, in the center
\usemintedstyle{tango}       % Colorscheme for minted

% PDF information and nice-looking urls
\hypersetup{%
  pdfauthor={David Oniani},
  pdftitle={Your Title Goes Here},
  pdfsubject={Subject 1, Subject 2, and Subject 3},
  pdfkeywords={Keyword 1, Keyword 2, Keyword 3},
  pdflang={English},
  colorlinks=true,
  linkcolor={black!50!blue},
  citecolor={black!50!blue},
  urlcolor={black!50!blue}
}

% Put a header
\chead{\footnotesize{Cosine Similarity and Its Applications in AI}}
\lhead{\footnotesize{David Oniani}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author(s), Title, and Date
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Author(s)
\author{David Oniani\\
        Luther College\\
        \href{mailto:oniada01@luther.edu}{oniada01@luther.edu}}

% Title
\title{\textbf{Cosine Similarity and Its Applications in AI}}

% Date
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of the Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
  \noindent Choosing the right metric~\cite{thomas2020} can be crucial to
  designing performant artificial intelligence models. Thousands of packages
  and libraries have been built and written just for providing these metrics.
  Cosine similarity is one of many metrics used extensively in natural language
  processing and artificial intelligence tasks. The paper will introduce the
  technique and discuss its advantages and disadvantages as well as compare it
  to other approaches. Additionally, sample implementations of the
  above-mentioned approaches will also be provided.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

There are plenty of approaches for comparing whether two texts are semantically
similar to each other. Cosine similarity is one of those methods. In order to
understand how cosine similarity works, let us first discuss the modeling of a
text document.

\subsection{Text Document Modeling}

There are several ways in which a text document can be modeled. This includes a
bag of words modeling, where the frequency of a term in a text document
represents its weight and therefore, more frequent words are deemed more
``important.'' Another example is \textit{tf-idf} vectorization. The whole idea
behind text document modeling is to transform the textual data into the numeric
data (usually, vectors). Once the numeric data is obtained, we can then apply
various text semantic similarity techniques in order to compare whether two
documents are similar to each other.

\subsection{Cosine Similarity}

Cosine similarity is a measure of similarity between two non-zero vectors of an
inner product space that measures the cosine of the angle between
them~\cite{wikicosine}. If we have two vectors \(\vec{a}\) and \(\vec{b}\),
then the cosine of these two vectors is
\(\vec{a} \cdot \vec{b} = \abs{\vec{a}}\abs{\vec{b}}\cos\theta\),
where \(\theta\) is the angle between these vectors. This is also known as the
Euclidean dot product formula.
The cosine of these two vectors can also be computed using their coordinates.
Therefore if
\(\vec{a} = \begin{bmatrix}a_1 \\ a_2\end{bmatrix}\) and
\(\vec{b} = \begin{bmatrix}b_1 \\ b_2\end{bmatrix}\), then
\(\vec{a} \cdot \vec{b} = a_1b_1 + a_2b_2\). Hence, for two vectors \(\vec{a}\)
and \(\vec{b}\) with coordinated \(a_1, a_2\) and \(b_1, b_2\) respectively,
the following holds:

\[\vec{a} \cdot \vec{b} = \abs{\vec{a}}\abs{\vec{b}}\cos\theta = a_1b_1 + a_2b_2\]

\bigskip

If the vector coordinates are known, then one can compute the magnitude of the
vector. For vector \(\vec{x} = \begin{bmatrix}x_1 \\ x_2 \\ \dots \\ x_n\end{bmatrix}\),
the magnitude is \(\sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}\). Hence, if we have two
vectors \(\vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ \dots \\ a_n\end{bmatrix}\) and
\(\vec{b} = \begin{bmatrix}b_1 \\ b_2 \\ \dots \\ b_n\end{bmatrix}\), then the cosine
of an angle between them can be computed using the formula:

\[ \cos\theta = \dfrac{a_1b_1 + a_2b_2 + \cdots + a_nb_n}{\sqrt{a_1^2 + a_2^2 + \cdots + a_n^2}\sqrt{b_1^2 + b_2^2 + \cdots + b_n^2}} \]

In other words, if we have two vectors with all their coordinates, it is
possible to compute the cosine similarity between them.

\bigskip

Some of you might be wondering where the cosine comes from. The answer lies in
the law of cosines. A great walkthrough is available on the proof Wikipedia
page~\cite{wikicosineproof}.

\subsection{Example}

As an example, consider two vectors
\(\vec{a} = \begin{bmatrix}3 \\ 4\end{bmatrix}\)
and \(\vec{b} = \begin{bmatrix}5 \\ 12\end{bmatrix}\). Then the magnitude of \(a\) is
\(\sqrt{1^2 + 2^4} = 5\) and the magnitude of \(b\) is \(\sqrt{3^2 + 4^2} = 13\).
Then \(\vec{a} \cdot \vec{b} = 15 + 48 = 63\) and
\(\abs{\vec{a}} \times \abs{\vec{b}} = 5 * 13 = 65\). Therefore, the cosine of
an angle between these vectors is \(\dfrac{63}{65} \approx 0.969\) which is the
similarity measure between these two vectors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Applications
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Applications}

Cosine similarity is one of the most commonly used approaches in text document
similarity.

\bigskip

Suppose that we have two documents \(D_1\) and \(D_2\) modeled as term vectors
\(\vec{t_1}\) and \(\vec{t_2}\) respectively. Then the similarity of two
documents corresponds to the correlation between the vectors and can be
quantified as a cosine of the angle between the vectors. The formula would be

\[SIM(D_1, D_2) = \dfrac{\vec{t_1} \cdot \vec{t_2}}{\abs{\vec{t_a}} \times
  \abs{\vec{t_b}}}.\]

As a result, the similarity value is non-negative, bounded by the closed
interval \([0,1]\).

\bigskip

It is important to note that the metric is a measurement of orientation and not
the magnitude. It is really a comparison of documents on a normalized space
since it does not only consider the magnitude of word counts of each document,
but also the angle between the documents.

\bigskip

Below find a Python implementation of cosine similarity.

\begin{figure}[H]
  \begin{minted}[fontsize=\small,frame=single]{python}
  import numpy as np

  def cosine_similarity(a: np.array, b: np.array) -> float:
      """Returns the cosine similarity value of two vectors."""

      dot_product: float = np.dot(a, b)
      magnitude_a: float = np.linalg.norm(a)
      magnitude_b: float = np.linalg.norm(b)
      similarity: float = dot_product / (magnitude_a * magnitude_b)

      return similarity


  if __name__ == "__main__":
      a: np.array = np.array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1])
      b: np.array = np.array([0, 2, 1, 0, 1, 3, 0, 0, 0, 1, 0])
      print(cosine_similarity(a, b))  # Prints out 0.5
  \end{minted}
  \caption{Sample Python Implementation of Cosine Similarity.}
\end{figure}

The program above is fairly simplistic (yet performant) and as shown, it takes
two vectors and prints out the cosine similarity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comparison to Other Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Performance and Comparison to Other Approaches}

Performance of cosine similarity can be altered by how the text document is
vectorized~\cite{sitikhu2019} (e.g., tf-idf vectorization might yield better
results than Word2Vec vectorization). This means that the right vectorization
strategy could drastically increase the performance, while the poor
vectorization will, unsurprisingly, result in a poor performance.

\medskip

There are many other approaches for finding semantic similarity between the
texts. The recent advancements in the domains of artificial intelligence and
natural language processing allowed for development of models such as
BERT~\cite{turc2019}, BioBERT~\cite{btz682}, and Universal Sentence Encoder
(USE)~\cite{use} can be used for high-accuracy text semantic similarity score
computations. These approaches are thought to, on average, outperform the
cosine similarity. That said, there are

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Current Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Current Work}

Cosine similarity is very much used in the modern, state-of-the-art papers,
such as ones cited in the paper. Its flexibility allows one to apply it in
virtually any setting, as long as the documents can be represented as vectors
(also known as term vectors). Therefore, cosine similarity is still a trendy
tool.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Summary
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

We have introduced cosine similarity and briefly covered its history. We have
discussed the importance of cosine similarity in the domains of artificial
intelligence and natural language processing and its prevalence in the
state-of-the-art work. Sample Python implementation of the algorithm was also
provided alongside with small illustrious examples depicting the cosine
similarity procedure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography and References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography[heading=bibintoc]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The End of the Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
