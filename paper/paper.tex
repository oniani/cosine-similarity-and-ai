%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Filename: template.tex
% Author:   David Oniani
% Modified: April 19, 2020
%  _         _____   __  __
% | |    __ |_   _|__\ \/ /
% | |   / _` || |/ _ \\  /
% | |__| (_| || |  __//  \
% |_____\__,_||_|\___/_/\_\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document Definition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages and Related Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Global, document-wide settings
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% Bibliography and references
\usepackage[backend=biber,sorting=none]{biblatex}
\addbibresource{references.bib}

% Other packages
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[cache=false]{minted}
\usepackage{tocloft}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Command Definitions and Redefinitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Operators
\DeclarePairedDelimiter\abs{\lvert}{\rvert}               % Absolute value
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}              % Ceiling
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}           % Floor

% New commands
\newcommand\und[1]{\underline{\smash{#1}}}  % Nice-looking underline
\renewcommand{\baselinestretch}{1.5}        % Line spacing is 1.5

% Rename "Contents" to "Table of Contents"
\addto\captionsenglish{% Replace "english" with the language used
  \renewcommand{\contentsname}%
    {\textbf{Table of Contents}}}%

% Filling the space for centering the title of the table of contents
% Dots for ToC sections
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cfttoctitlefont}{\hspace*{\fill}\Large}
\renewcommand{\cftaftertoctitle}{\hspace*{\fill}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Miscellaneous
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Setting stuff
\setlength{\parindent}{0pt}  % Remove indentations from paragraphs
\pagestyle{fancy}            % This allows to do fancy headers and footers
\fancyhf{}                   % No additional page numbering (or other stuff)
\cfoot{\thepage}             % Display page number at the bottom, in the center
\usemintedstyle{tango}       % Colorscheme for minted

% PDF information and nice-looking urls
\hypersetup{%
  pdfauthor={David Oniani},
  pdftitle={Your Title Goes Here},
  pdfsubject={Subject 1, Subject 2, and Subject 3},
  pdfkeywords={Keyword 1, Keyword 2, Keyword 3},
  pdflang={English},
  colorlinks=true,
  linkcolor={black!50!blue},
  citecolor={black!50!blue},
  urlcolor={black!50!blue}
}

% Put a header
\chead{\footnotesize{Cosine Similarity and Its Applications in AI}}
\lhead{\footnotesize{David Oniani}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author(s), Title, and Date
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Author(s)
\author{David Oniani\\
        Luther College\\
        \href{mailto:oniada01@luther.edu}{oniada01@luther.edu}}

% Title
\title{\textbf{Cosine Similarity and Its Applications in AI}}

% Date
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of the Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
  \noindent Choosing the right metric~\cite{thomas2020} can be crucial to
  designing performant artificial intelligence models. Thousands of packages
  and libraries have been built and written just for providing these metrics.
  Cosine similarity is one of many metrics used extensively in natural language
  processing and artificial intelligence tasks. The paper will introduce the
  technique and discuss its advantages and disadvantages as well as compare it
  to other approaches. Additionally, sample implementations of the
  above-mentioned approaches will also be provided.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

There are a number of approaches for comparing whether two texts are
semantically similar to each other. Cosine similarity is one of those methods.
In order to understand how cosine similarity works, let us first discuss the
modeling of a text document and cosine similarity in general and then proceed
by its applications in text document similarity tasks.

\subsection{Document Modeling}

There are several ways in which a text document can be modeled. This includes a
bag of words modeling, where the frequency of a term in a text document
represents its weight and therefore, more frequent words are deemed more
``important.'' The whole idea behind a text document modeling is to quantify
the textual data into the numeric data (usually, vectors). Once the numeric
data is obtained, we can then apply various text semantic similarity techniques
in order to compare whether two documents are similar to each other.

\subsection{Cosine Similarity}

Cosine similarity is a measure of similarity between two non-zero vectors of an
inner product space that measures the cosine of the angle between
them~\cite{wikicosine}. If we have two vectors \(\vec{a}\) and \(\vec{b}\),
then the cosine of these two vectors is \(\vec{a} \cdot \vec{b}\) which is
equal to \(\left\|\vec{a}\right\|\left\|\vec{b}\right\|\cos\theta\) where
\(\theta\) is the angle between these vectors (Euclidean dot product formula).
It can also be represented as the product of two vectors. Therefore if
\(\vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ a_3\end{bmatrix}\) and
\(\vec{b} = \begin{bmatrix}b_1 \\ b_2 \\ b_3\end{bmatrix}\), then
\(\vec{a} \cdot \vec{b}\) is also equal to \(a_1b_1 + a_2b_2 + a_3b_3\).

\bigskip

Some of you might be wondering where the cosine comes from. The answer lies in
the law of cosines. A great walkthrough is available on the proof Wikipedia
page~\cite{wikicosineproof}.

\subsection{Example}

As an example, consider two vectors
\(\vec{a} = \begin{bmatrix}3 \\ 4\end{bmatrix}\)
and \(\vec{b} = \begin{bmatrix}5 \\ 12\end{bmatrix}\). Then the size of \(a\) is
\(\sqrt{1^2 + 2^4} = 5\) and the size of \(b\) is \(\sqrt{3^2 + 4^2} = 13\).
Then \(\vec{a} \cdot \vec{b} = 15 + 48 = 63\) and
\(\abs{\vec{a}} \times \abs{\vec{b}} = 5 * 13 = 65\). Therefore, the cosine of
an angle between these vectors is \(\dfrac{63}{65}\) which is the similarity
measure between these two vectors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Applications
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Applications}

Cosine similarity is one of the most commonly used approaches in text document
similarity.

\bigskip

Suppose that we have two documents \(D_1\) and \(D_2\) modeled as term vectors
\(\vec{t_1}\) and \(\vec{t_2}\) respectively. Then the similarity of two
documents corresponds to the correlation between the vectors and can be
quantified as a cosine of the angle between the vectors. The formula would be

\[SIM(D_1, D_2) = \dfrac{\vec{t_1} \cdot \vec{t_2}}{\abs{\vec{t_a}} \times
  \abs{\vec{t_b}}}.\]

As a result, the similarity value is non-negative, bounded by the closed
interval \([0,1]\).

\bigskip

Below find a Python implementation of cosine similarity.

\begin{figure}[h]
  \begin{minted}[fontsize=\small,frame=single]{python}
  import numpy as np

  def cosine_similarity(a: np.array, b: np.array) -> float:
      """Returns the cosine similarity value of two vectors."""

      dot_product: float = np.dot(a, b)
      size_a: float = np.linalg.norm(a)
      size_b: float = np.linalg.norm(b)
      similarity: float = dot_product / (size_a * size_b)

      return similarity


  if __name__ == "__main__":
      a: np.array = np.array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1])
      b: np.array = np.array([0, 2, 1, 0, 1, 3, 0, 0, 0, 1, 0])
      print(cosine_similarity(a, b))  # Prints out 0.5
  \end{minted}
  \caption{Sample Python Implementation of Cosine Similarity.}
\end{figure}

The program above is fairly simplistic (yet performant) and as shown, it takes
two vectors and prints out the cosine similarity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comparison to Other Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Performance and Comparison to Other Approaches}

Performance of cosine similarity can be altered by how the text document is
vectorized~\cite{sitikhu2019} (e.g., tf-idf vectorization might yield better
results than Word2Vec vectorization). This means that the right vectorization
strategy could drastically increase the performance, while the poor
vectorization will, unsurprisingly, result in a poor performance.

\medskip

There are many other approaches for finding semantic similarity between the
texts. The recent advancements in the domains of artificial intelligence and
natural language processing allowed for development of models such as
BERT~\cite{turc2019}, BioBERT~\cite{btz682}, and Universal Sentence Encoder
(USE)~\cite{use} can be used for high-accuracy text semantic similarity score
computations. These approaches are thought to, on average, outperform the
cosine similarity. That said, there are

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Current Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Current Work}

Cosine similarity is very much used in the modern, state-of-the-art papers,
such as ones cited in the paper. Its flexibility allows one to apply it in
virtually any setting, as long as the documents can be represented as vectors
(also known as term vectors). Therefore, cosine similarity is still a trendy
tool.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Summary
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

We have introduced cosine similarity and briefly covered its history. We have
discussed the importance of cosine similarity in the domains of artificial
intelligence and natural language processing and its prevalence in the
state-of-the-art work. Sample Python implementation of the algorithm was also
provided alongside with small illustrious examples depicting the cosine
similarity procedure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography and References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography[heading=bibintoc]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The End of the Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
